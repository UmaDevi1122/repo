# repo

Dataphion Assignment

The task at hand involves developing a Spark job to ingest events from Apache Kafka and store them in Delta Lake tables. The tables will consist of essential columns such as Event Name, Type, Value, Timestamp, Page Source, URL, Component ID, User ID, and Date. Subsequently, the job will be responsible for generating daily, weekly, and monthly reports. These reports will include insights such as the top 10 events and their respective counts for a specified date, the count of returning users, active users, and churn analysis. This assignment aims to streamline event data processing, facilitate insightful reporting, and provide actionable analytics for better decision-making based on user interactions.
